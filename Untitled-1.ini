{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация ответов на вопросы с помощью LLM\n",
    "\n",
    "В этом ноутбуке мы рассмотрим, как использовать большие языковые модели (LLM) для генерации ответов на вопросы.\n",
    "\n",
    "Мы изучим:\n",
    "- Подходы к задаче генерации ответов.\n",
    "- Подготовку данных.\n",
    "- Настройку модели (инференс и файнтюнинг).\n",
    "- Примеры с использованием `transformers` и `Hugging Face`.\n",
    "- Методы улучшения качества генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установим необходимые библиотеки\n",
    "!pip install transformers datasets torch accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, pipeline,\n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подходы к задаче генерации ответов\n",
    "\n",
    "Существует несколько способов решать задачу генерации ответов:\n",
    "\n",
    "### 1.1. Zero-shot генерация\n",
    "- Модель не дообучается.\n",
    "- Используется промпт вида: `\"Question: ...\\nAnswer:\"`.\n",
    "- Подходит для быстрого прототипа.\n",
    "\n",
    "### 1.2. Fine-tuning модели на датасете вопрос-ответ\n",
    "- Модель дообучается на парах (вопрос, ответ).\n",
    "- Повышает точность и стилистическую согласованность.\n",
    "\n",
    "### 1.3. Использование инструкций (Instruction Tuning)\n",
    "- Промпты формируются как: `\"[INST] Question: ... [/INST] Answer: ...\"`.\n",
    "- Подходит для инструкционно-настроенных моделей (например, `alpaca`, `llama-2-chat`).\n",
    "\n",
    "### 1.4. Retrieval-Augmented Generation (RAG)\n",
    "- Поиск релевантного контекста перед генерацией.\n",
    "- Повышает достоверность ответа.\n",
    "\n",
    "Мы сфокусируемся на подходах 1.1 и 1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Подготовка данных\n",
    "\n",
    "Создадим небольшой датасет для примера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"question\": [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Who wrote '1984'?\",\n",
    "        \"How does photosynthesis work?\",\n",
    "        \"What is Python used for?\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"The capital of France is Paris.\",\n",
    "        \"George Orwell wrote '1984'.\",\n",
    "        \"Photosynthesis is the process by which green plants use sunlight to synthesize foods from carbon dioxide and water.\",\n",
    "        \"Python is used for web development, data analysis, artificial intelligence, and more.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Формирование текста для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa(example):\n",
    "    return {\"text\": f\"Question: {example['question']}\\nAnswer: {example['answer']}\"}\n",
    "\n",
    "dataset = dataset.map(format_qa)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-shot генерация (без файнтюна)\n",
    "\n",
    "Выберем модель, например `gpt2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt, model, tokenizer):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[-1] + 100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Question: What is quantum computing?\\nAnswer:\"\n",
    "generated = generate_answer(prompt, model, tokenizer)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning модели на датасете вопрос-ответ\n",
    "\n",
    "Теперь дообучим модель на нашем датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация датасета\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qa_model_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    report_to=None,  # отключить логирование в wandb/mlflow\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    logging_first_step=True,\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available(),  # если GPU поддерживает\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним модель\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(\"./qa_model_finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Проверка дообученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим дообученную модель\n",
    "fine_model = AutoModelForCausalLM.from_pretrained(\"./qa_model_finetuned\")\n",
    "fine_tokenizer = AutoTokenizer.from_pretrained(\"./qa_model_finetuned\")\n",
    "fine_tokenizer.pad_token = fine_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Question: What is the capital of France?\\nAnswer:\"\n",
    "generated = generate_answer(prompt, fine_model, fine_tokenizer)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Методы улучшения генерации\n",
    "\n",
    "- **Использование инструкций**: Форматировать промпты как `[INST] ... [/INST]`.\n",
    "- **LoRA/QLoRA**: Для эффективного файнтюна больших моделей.\n",
    "- **RAG**: Использовать векторные базы (Pinecone, FAISS) для поиска контекста.\n",
    "- **Post-processing**: Фильтрация, проверка на достоверность (fact-checking).\n",
    "- **Prompt Engineering**: Использовать более сложные и структурированные промпты.\n",
    "- **Контроль длины и температуры**: Настройка параметров генерации для стабильности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Выводы\n",
    "\n",
    "Мы рассмотрели:\n",
    "- Как генерировать ответы на вопросы с помощью LLM.\n",
    "- Как дообучить модель на датасете вопрос-ответ.\n",
    "- Различные подходы к задаче и методы улучшения.\n",
    "\n",
    "Этот шаблон можно адаптировать под ваши данные и задачи."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}