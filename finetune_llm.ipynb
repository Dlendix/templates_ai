{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# 1. Загрузка модели и токенизатора\n",
    "model_name = \"gpt2\"  # Можно заменить на другую модель, например 'microsoft/DialoGPT-medium'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Добавим токен для padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Пример генерации с использованием pipeline\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Пример промпта\n",
    "prompt = \"Question: What is the capital of France?\\nAnswer:\"\n",
    "\n",
    "# Генерация\n",
    "generated = generator(\n",
    "    prompt,\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"Сгенерированный ответ:\")\n",
    "print(generated[0]['generated_text'])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 3. Ручная генерация с контролем параметров\n",
    "def generate_answer(prompt, model, tokenizer, max_new_tokens=50):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[-1] + max_new_tokens,\n",
    "            temperature=0.8,      # Контроль \"креативности\"\n",
    "            top_p=0.9,            # Nucleus sampling\n",
    "            top_k=50,             # Ограничение на топ-K\n",
    "            do_sample=True,       # Использовать вероятностный выбор\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Пример с другим вопросом\n",
    "prompt2 = \"Question: How does photosynthesis work?\\nAnswer:\"\n",
    "generated_text = generate_answer(prompt2, model, tokenizer)\n",
    "print(\"Сгенерированный ответ (ручная генерация):\")\n",
    "print(generated_text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 4. Подготовка датасета для дообучения (пример)\n",
    "# В реальности вы загрузите свой датасет (например, CSV с колонками 'question', 'answer')\n",
    "data = [\n",
    "    {\"question\": \"What is the capital of Japan?\", \"answer\": \"Tokyo is the capital of Japan.\"},\n",
    "    {\"question\": \"Who invented the telephone?\", \"answer\": \"Alexander Graham Bell invented the telephone.\"},\n",
    "    {\"question\": \"What is 2+2?\", \"answer\": \"2+2 equals 4.\"}\n",
    "]\n",
    "\n",
    "# Формирование текста для обучения\n",
    "formatted_data = [f\"Question: {item['question']}\\nAnswer: {item['answer']}\" for item in data]\n",
    "\n",
    "# 5. Пример дообучения (fine-tuning) на этих данных (упрощённо)\n",
    "# Для полноценного дообучения потребуется больше шагов: токенизация, датасет, тренер и т.д.\n",
    "# Ниже — минимальный пример подготовки данных\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# Для простоты сохраним данные в файл\n",
    "with open(\"train_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in formatted_data:\n",
    "        f.write(text + \"\\n\\n\")\n",
    "\n",
    "# Загрузим датасет из файла\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"train_data.txt\",\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Языковая модель обучается на следующий токен, а не на маскировку\n",
    ")\n",
    "\n",
    "print(\"Датасет для дообучения подготовлен.\")\n",
    "print(\"Для полноценного дообучения используйте Trainer из transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac80e01",
   "metadata": {},
   "source": [
    "Цикл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Загрузка модели и токенизатора\n",
    "model_name = \"gpt2\"  # или любая другая подходящая модель\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Убедимся, что токен для padding задан\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Функция генерации ответа\n",
    "def generate_answer(prompt, model, tokenizer, max_new_tokens=100):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[-1] + max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Пример списка вопросов (замените его на ваш список)\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote '1984'?\",\n",
    "    \"How does photosynthesis work?\",\n",
    "    # ... и так далее, до 300 вопросов\n",
    "]\n",
    "\n",
    "# Список для хранения результатов\n",
    "results = []\n",
    "\n",
    "# Цикл по всем вопросам\n",
    "for i, question in enumerate(questions):\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    generated_text = generate_answer(prompt, model, tokenizer)\n",
    "    \n",
    "    # Извлечение только ответа (опционально)\n",
    "    answer = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"generated_answer\": answer\n",
    "    })\n",
    "    \n",
    "    # Опционально: вывод прогресса\n",
    "    print(f\"[{i+1}/{len(questions)}] Question: {question}\")\n",
    "    print(f\"Generated Answer: {answer}\\n\")\n",
    "\n",
    "# Теперь у вас есть список словарей с вопросами и ответами\n",
    "# Вы можете сохранить его в CSV, JSON и т.д.\n",
    "import json\n",
    "\n",
    "with open(\"generated_answers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Ответы сохранены в файл 'generated_answers.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d1c94",
   "metadata": {},
   "source": [
    "батчинг + лайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8701ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    batch_size=4  # Обрабатывать по 4 промпта за раз\n",
    ")\n",
    "\n",
    "prompts = [f\"Question: {q}\\nAnswer:\" for q in questions]\n",
    "\n",
    "# Генерация для всех промптов\n",
    "generated_batch = generator(\n",
    "    prompts,\n",
    "    max_length=200,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Извлечение ответов\n",
    "results = []\n",
    "for i, gen in enumerate(generated_batch):\n",
    "    full_text = gen['generated_text']\n",
    "    answer = full_text[len(prompts[i]):].strip()\n",
    "    results.append({\n",
    "        \"question\": questions[i],\n",
    "        \"generated_answer\": answer\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10497427",
   "metadata": {},
   "source": [
    "файнтюн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22265532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Пример данных\n",
    "data = {\n",
    "    \"question\": [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Who invented the telephone?\",\n",
    "        \"How does photosynthesis work?\",\n",
    "        \"What is Python used for?\",\n",
    "        \"Who wrote 'Romeo and Juliet'?\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"The capital of France is Paris.\",\n",
    "        \"Alexander Graham Bell invented the telephone.\",\n",
    "        \"Photosynthesis is the process by which green plants use sunlight to synthesize food.\",\n",
    "        \"Python is used for web development, data science, AI, and more.\",\n",
    "        \"William Shakespeare wrote 'Romeo and Juliet'.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def format_qa(example):\n",
    "    return {\"text\": f\"Question: {example['question']}\\nAnswer: {example['answer']}\"}\n",
    "\n",
    "dataset = dataset.map(format_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qa_model_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_dir='./logs',\n",
    "    report_to=None,\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(\"./qa_model_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13376eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "model_name = \"microsoft/DialoGPT-medium\"  # или любая другая модель\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Настройка LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"]  # для GPT-2 / GPT-3\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_qa_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-4,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_dir='./logs',\n",
    "    report_to=None,\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(\"./lora_qa_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Токенизация\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "model.save_pretrained(\"./manual_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./manual_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2aefa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
