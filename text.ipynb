{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d337b213",
   "metadata": {},
   "source": [
    "Sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7732e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# === Пример 1: NER (сегментация по сущностям) ===\n",
    "print(\"=== Пример 1: NER ===\")\n",
    "nlp_ner = pipeline(\"ner\", model=\"Davlan/bert-base-multilingual-cased-ner-hrl\", aggregation_strategy=\"simple\")\n",
    "text = \"Apple was founded by Steve Jobs. It is located in Cupertino.\"\n",
    "entities = nlp_ner(text)\n",
    "for ent in entities:\n",
    "    print(ent)\n",
    "\n",
    "# === Пример 2: Summarization (сегментация на логические блоки) ===\n",
    "print(\"\\n=== Пример 2: Summarization ===\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "text_sum = \"Machine learning is a field of artificial intelligence. It allows systems to learn from data. Natural language processing is used to understand human language.\"\n",
    "sentences = re.split(r'[.!?]+', text_sum)\n",
    "sentences = [s.strip() for s in sentences if s.strip()]\n",
    "for i, sent in enumerate(sentences):\n",
    "    if len(sent) > 20:\n",
    "        summary = summarizer(sent, max_length=20, min_length=5, do_sample=False)\n",
    "        print(f\"Блок {i+1}: {summary[0]['summary_text']}\")\n",
    "\n",
    "# === Пример 3: Кластеризация по смыслу (sentence transformers) ===\n",
    "print(\"\\n=== Пример 3: Кластеризация по смыслу ===\")\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "text_cluster = \"I love machine learning. It's fascinating how algorithms work. I also enjoy hiking and spending time outdoors.\"\n",
    "sentences = re.split(r'[.!?]+', text_cluster)\n",
    "sentences = [s.strip() for s in sentences if s.strip()]\n",
    "embeddings = model.encode(sentences)\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"Кластер {clusters[i]}: {sent}\")\n",
    "\n",
    "# === Пример 4: Zero-shot classification (сегментация по темам) ===\n",
    "print(\"\\n=== Пример 4: Zero-shot classification ===\")\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "candidate_labels = [\"technology\", \"personal life\", \"work\"]\n",
    "text_zs = \"I love machine learning. Nature is my escape from work.\"\n",
    "sentences = re.split(r'[.!?]+', text_zs)\n",
    "sentences = [s.strip() for s in sentences if s.strip()]\n",
    "for sent in sentences:\n",
    "    if len(sent) > 10:\n",
    "        result = classifier(sent, candidate_labels)\n",
    "        print(f\"Текст: {sent}\")\n",
    "        print(f\"Тема: {result['labels'][0]}, Вероятность: {result['scores'][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "\n",
    "# Загрузка модели spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Пример текста (например, транскрипт интервью)\n",
    "text = \"\"\"\n",
    "    I love machine learning. It's fascinating how algorithms work.\n",
    "    On the other hand, coding can be exhausting sometimes.\n",
    "    I also enjoy hiking and spending time outdoors.\n",
    "    Deep learning models are very powerful.\n",
    "    But debugging code is not fun.\n",
    "    Nature is my escape from work.\n",
    "\"\"\"\n",
    "\n",
    "# Шаг 1: Разделение на предложения\n",
    "doc = nlp(text)\n",
    "sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "# Шаг 2: Предобработка текста (токенизация, удаление стоп-слов и лемматизация)\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_.lower() for token in doc\n",
    "            if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "\n",
    "processed = [preprocess(sent) for sent in sentences]\n",
    "\n",
    "# Шаг 3: Создание словаря и корпуса\n",
    "dictionary = corpora.Dictionary(processed)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed]\n",
    "\n",
    "# Шаг 4: Обучение LDA модели\n",
    "num_topics = 2\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    passes=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Шаг 5: Присвоение тем каждому предложению\n",
    "topics_per_sent = [lda_model[corpus[i]] for i in range(len(corpus))]\n",
    "\n",
    "# Вывод\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"Предложение: {sent}\")\n",
    "    print(f\"Темы: {topics_per_sent[i]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03caaf73",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707fe199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Пример данных\n",
    "texts = [\n",
    "    \"я люблю машинное обучение\",\n",
    "    \"модели глубокого обучения потрясают\",\n",
    "    \"я не люблю писать код\",\n",
    "    \"программирование — это весело\",\n",
    "    \"нейросети меняют мир\",\n",
    "    \"не могу понять, как работает TF-IDF\"\n",
    "]\n",
    "labels = [1, 1, 0, 1, 1, 0]  # 1 — позитивный тон, 0 — негативный\n",
    "\n",
    "# Векторизация текста\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    stop_words=None,  # можно указать 'english' или кастомные стоп-слова\n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Обучение модели\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Преобразование в датафрейм (опционально)\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4079b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
